"""
PDF Belge AsistanÄ± - Google Gemini Edition (Optimize EdilmiÅŸ)
KullanÄ±cÄ±larÄ±n PDF dosyasÄ± yÃ¼kleyip sorular sorabileceÄŸi bir Streamlit uygulamasÄ±.
"""

import streamlit as st
from PyPDF2 import PdfReader
import google.generativeai as genai
import os
from dotenv import load_dotenv
import json
from datetime import datetime
import time

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# Sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="PDF Belge AsistanÄ±",
    page_icon="ğŸ“„",
    layout="wide"
)

from i18n import get_translation


def t(key, **kwargs):
    lang = st.session_state.get("selected_language", "tr")
    return get_translation(lang, key, **kwargs)


# BaÅŸlÄ±k ve aÃ§Ä±klama (localize edilmiÅŸ)
st.title(t("title"))
st.markdown(t("description"))


def extract_text_from_pdf(pdf_file):
    """
    PDF dosyasÄ±ndan metin Ã§Ä±karÄ±r.
    
    Args:
        pdf_file: YÃ¼klenen PDF dosyasÄ±
        
    Returns:
        tuple: (metin, sayfa_sayÄ±sÄ±)
    """
    try:
        pdf_reader = PdfReader(pdf_file)
        text = ""
        page_count = len(pdf_reader.pages)
        
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            if page_text.strip():  # BoÅŸ sayfalarÄ± atla
                text += f"\n--- Sayfa {page_num} ---\n{page_text}"
        
        return text, page_count
    except Exception as e:
        st.error(f"PDF okunurken hata oluÅŸtu: {str(e)}")
        return None, 0


def get_text_stats(text):
    """
    Metin istatistiklerini hesaplar.
    
    Args:
        text: Analiz edilecek metin
        
    Returns:
        dict: Karakter ve kelime sayÄ±sÄ±
    """
    word_count = len(text.split())
    char_count = len(text)
    return {"words": word_count, "characters": char_count}


def chunk_text(text, max_chars=3000):
    """
    Metni kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler (token tasarrufu iÃ§in).
    
    Args:
        text: BÃ¶lÃ¼necek metin
        max_chars: Maksimum karakter sayÄ±sÄ±
        
    Returns:
        list: Metin parÃ§alarÄ±
    """
    chunks = []
    current_chunk = ""
    
    for line in text.split('\n'):
        if len(current_chunk) + len(line) < max_chars:
            current_chunk += line + '\n'
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = line + '\n'
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


def search_relevant_chunks(chunks, query, top_k=2):
    """
    Soruyla ilgili en alakalÄ± metin parÃ§alarÄ±nÄ± bulur (basit keyword arama).
    
    Args:
        chunks: Metin parÃ§alarÄ± listesi
        query: KullanÄ±cÄ± sorusu
        top_k: KaÃ§ parÃ§a dÃ¶ndÃ¼rÃ¼lecek
        
    Returns:
        str: BirleÅŸtirilmiÅŸ alakalÄ± metin parÃ§alarÄ±
    """
    query_words = set(query.lower().split())
    
    # Her chunk iÃ§in skor hesapla
    scored_chunks = []
    for chunk in chunks:
        chunk_words = set(chunk.lower().split())
        score = len(query_words & chunk_words)  # Ortak kelime sayÄ±sÄ±
        scored_chunks.append((score, chunk))
    
    # En yÃ¼ksek skorlu parÃ§alarÄ± al
    scored_chunks.sort(reverse=True, key=lambda x: x[0])
    relevant_chunks = [chunk for score, chunk in scored_chunks[:top_k] if score > 0]
    
    # EÄŸer hiÃ§ eÅŸleÅŸme yoksa ilk chunk'Ä± dÃ¶ndÃ¼r
    if not relevant_chunks and chunks:
        relevant_chunks = [chunks[0]]
    
    return '\n\n'.join(relevant_chunks)


def initialize_gemini(model_name, api_key):
    """
    Google Gemini modelini baÅŸlatÄ±r.
    
    Args:
        model_name: KullanÄ±lacak Gemini model adÄ±
        api_key: Google API anahtarÄ±
        
    Returns:
        GenerativeModel: YapÄ±landÄ±rÄ±lmÄ±ÅŸ Gemini modeli
    """
    try:
        genai.configure(api_key=api_key)
        # Model adÄ±na "models/" prefix'i ekle
        full_model_name = f"models/{model_name}" if not model_name.startswith("models/") else model_name
        
        # Optimized generation config
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 2048,  # Ã‡Ä±kÄ±ÅŸ token limiti
        }
        
        model = genai.GenerativeModel(
            full_model_name,
            generation_config=generation_config
        )
        return model
    except Exception as e:
        st.error(f"Model baÅŸlatÄ±lÄ±rken hata: {str(e)}")
        return None


def get_gemini_response(model, prompt, pdf_chunks, chat_history):
    """
    Gemini'den yanÄ±t alÄ±r (Optimize EdilmiÅŸ - Daha Az Token).
    
    Args:
        model: Gemini model instance
        prompt: KullanÄ±cÄ± sorusu
        pdf_chunks: PDF iÃ§eriÄŸi parÃ§alarÄ±
        chat_history: Sohbet geÃ§miÅŸi
        
    Returns:
        str: Model yanÄ±tÄ±
    """
    try:
        # Soruyla ilgili en alakalÄ± metinleri bul
        relevant_context = search_relevant_chunks(pdf_chunks, prompt, top_k=2)
        
        # Sadece son 2 sohbet turunu dahil et (token tasarrufu)
        recent_history = chat_history[-4:] if len(chat_history) > 4 else chat_history
        
        # KÄ±sa chat history formatla
        history_text = ""
        if recent_history:
            for msg in recent_history:
                role = "K" if msg["role"] == "user" else "A"
                # Uzun mesajlarÄ± kÄ±salt
                content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
                history_text += f"{role}: {content}\n"
        
        # KÄ±saltÄ±lmÄ±ÅŸ ve optimize edilmiÅŸ prompt
        system_prompt = """PDF belge asistanÄ±sÄ±n. Sadece verilen bilgilere gÃ¶re yanÄ±t ver.

Ä°lgili Metin:
{context}

{history}
Soru: {question}

YanÄ±t:"""

        # Prompt'u hazÄ±rla
        full_prompt = system_prompt.format(
            context=relevant_context[:3500],  # Daha az token
            history=f"Ã–nceki:\n{history_text}\n" if history_text else "",
            question=prompt
        )
        
        # GÃ¼venlik ayarlarÄ±
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
        ]
        
        # Rate limiting - her istekten Ã¶nce kÄ±sa bir bekleme
        if 'last_request_time' in st.session_state:
            elapsed = time.time() - st.session_state.last_request_time
            if elapsed < 2:  # 2 saniyeden kÄ±sa sÃ¼rede istek atÄ±lmÄ±ÅŸsa bekle
                time.sleep(2 - elapsed)
        
        st.session_state.last_request_time = time.time()
        
        # Gemini'den yanÄ±t al
        response = model.generate_content(
            full_prompt,
            safety_settings=safety_settings
        )
        
        return response.text
    
    except Exception as e:
        raise Exception(f"Gemini yanÄ±t hatasÄ±: {str(e)}")


def export_chat_history(messages, format_type="txt"):
    """
    Sohbet geÃ§miÅŸini dÄ±ÅŸa aktarÄ±r.
    
    Args:
        messages: Sohbet mesajlarÄ± listesi
        format_type: Dosya formatÄ± ("txt" veya "json")
        
    Returns:
        str: DÄ±ÅŸa aktarÄ±lacak iÃ§erik
    """
    if format_type == "txt":
        content = "PDF Belge AsistanÄ± - Sohbet GeÃ§miÅŸi\n"
        content += "=" * 50 + "\n"
        content += f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        for msg in messages:
            role = "KullanÄ±cÄ±" if msg["role"] == "user" else "Asistan"
            content += f"{role}: {msg['content']}\n\n"
        
        return content
    
    elif format_type == "json":
        export_data = {
            "export_date": datetime.now().isoformat(),
            "messages": messages
        }
        return json.dumps(export_data, ensure_ascii=False, indent=2)


# Session state baÅŸlatma
if "messages" not in st.session_state:
    st.session_state.messages = []

if "pdf_text" not in st.session_state:
    st.session_state.pdf_text = None

if "pdf_chunks" not in st.session_state:
    st.session_state.pdf_chunks = []

if "pdf_info" not in st.session_state:
    st.session_state.pdf_info = {}

if "gemini_model" not in st.session_state:
    st.session_state.gemini_model = None

if "last_request_time" not in st.session_state:
    st.session_state.last_request_time = 0


# Sidebar - Ayarlar ve Kontroller
with st.sidebar:
    st.header(t("settings"))

    # Dil seÃ§imi
    lang_default = st.session_state.get("selected_language", "tr")
    lang_choice = st.selectbox(
        t("language_label"),
        options=[("tr", "TÃ¼rkÃ§e"), ("en", "English")],
        format_func=lambda x: x[1],
        index=0 if lang_default == "tr" else 1,
    )
    st.session_state.selected_language = lang_choice[0]

    # API Key kontrolÃ¼ - GEMINI
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        api_key = st.text_input(
            t("api_key_input"),
            type="password",
            help=t("api_key_help")
        )

    if api_key:
        st.success(t("api_key_loaded"))
    else:
        st.warning(t("api_key_missing"))

    # Model seÃ§imi - GÃœNCEL GEMINI MODELLER
    st.subheader("ğŸ¤– " + t("model_selection"))

    # GÃ¼ncel Gemini model kategorileri ve aÃ§Ä±klamalarÄ±
    model_info = {
        "gemini-flash-latest": "ğŸ’¨ Ultra hafif - En az token (Ã–NERÄ°LEN)",
        "gemini-1.5-flash": "âš¡ HÄ±zlÄ± ve dengeli",
        "gemini-2.0-flash-exp": "ğŸš€ Yeni deneysel model",
        "gemini-1.5-pro": "ğŸ’ En gÃ¼Ã§lÃ¼ (daha fazla token)"
    }

    selected_model = st.selectbox(
        t("model_selection"),
        list(model_info.keys()),
        index=0,
        format_func=lambda x: f"{x} - {model_info[x]}",
        help="Quota sorunu iÃ§in gemini-1.5-flash-8b Ã¶nerilir"
    )

    # Model bilgisi
    st.info(t("selected_model_info", model=selected_model))

    # Optimizasyon bilgisi
    with st.expander(t("optimization_notes")):
        st.markdown(t("optimization_content"))

    # API Key alma bilgisi
    with st.expander(t("how_to_get_key")):
        st.markdown(t("how_to_get_key_steps"))

    st.divider()

    # PDF yÃ¼kleme
    st.subheader("ğŸ“¤ " + t("upload_pdf"))
    uploaded_file = st.file_uploader(
        t("upload_pdf"),
        type=["pdf"],
        help=t("upload_help")
    )

    # Dosya boyutu kontrolÃ¼
    if uploaded_file is not None:
        file_size_mb = uploaded_file.size / (1024 * 1024)

        if file_size_mb > 10:
            st.error(t("file_too_large"))
            uploaded_file = None
        else:
            st.info(t("file_size_info", size=f"{file_size_mb:.2f}"))

            # PDF iÅŸleme
            if st.button(t("process_pdf"), type="primary"):
                with st.spinner("PDF okunuyor..."):
                    text, page_count = extract_text_from_pdf(uploaded_file)

                    if text:
                        st.session_state.pdf_text = text

                        # Metni parÃ§alara bÃ¶l
                        with st.spinner("Metin parÃ§alanÄ±yor..."):
                            chunks = chunk_text(text, max_chars=3000)
                            st.session_state.pdf_chunks = chunks

                        st.session_state.pdf_info = {
                            "filename": uploaded_file.name,
                            "pages": page_count,
                            "stats": get_text_stats(text),
                            "chunks": len(chunks)
                        }

                        # Gemini modelini baÅŸlat
                        if api_key:
                            with st.spinner(f"{selected_model} baÅŸlatÄ±lÄ±yor..."):
                                model = initialize_gemini(selected_model, api_key)
                                if model:
                                    st.session_state.gemini_model = model
                                    st.success(f"âœ… PDF yÃ¼klendi! ({page_count} sayfa, {len(chunks)} parÃ§a)")
                                else:
                                    st.error("âŒ Model baÅŸlatÄ±lamadÄ±. API Key'inizi kontrol edin.")
                        else:
                            st.error(t("api_key_missing"))

                        if st.session_state.gemini_model:
                            st.rerun()

    # PDF bilgileri
    if st.session_state.pdf_text:
        st.divider()
        st.subheader(t("document_info"))
        st.write(f"**{t('file_label')}** {st.session_state.pdf_info['filename']}")
        st.write(f"**{t('pages_label')}** {st.session_state.pdf_info['pages']}")
        st.write(f"**{t('word_count')}** {st.session_state.pdf_info['stats']['words']:,}")
        st.write(f"**{t('chunks_label')}** {st.session_state.pdf_info['chunks']}")

        # Token tahmini
        estimated_tokens = st.session_state.pdf_info['stats']['characters'] // 4
        st.write(f"**{t('estimated_tokens')}** ~{estimated_tokens:,}")

        # PDF Ã¶nizleme
        with st.expander(t("preview_label")):
            preview_text = st.session_state.pdf_text[:500] + "..."
            st.text_area(t('first_500_chars'), preview_text, height=150, disabled=True)

    # Sohbet kontrolÃ¼
    if st.session_state.messages:
        st.divider()
        st.subheader(t("chat_control"))

        st.info(t("chat_count_info", count=len(st.session_state.messages)))

        # Sohbeti temizle
        if st.button(t("clear_chat"), type="secondary"):
            st.session_state.messages = []
            st.rerun()

        # Sohbeti indir
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                label=t("download_txt"),
                data=export_chat_history(st.session_state.messages, "txt"),
                file_name=f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                use_container_width=True
            )
        with col2:
            st.download_button(
                label=t("download_json"),
                data=export_chat_history(st.session_state.messages, "json"),
                file_name=f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )


# Ana alan - Sohbet
if not st.session_state.pdf_text:
    st.info(t("start_hint"))
elif not st.session_state.gemini_model:
    st.warning(t("model_not_started"))
else:
    # Sohbet geÃ§miÅŸini gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # KullanÄ±cÄ± giriÅŸi
    if prompt := st.chat_input(t("chat_placeholder")):
        if not api_key:
            st.error(t("api_key_missing"))
        else:
            # KullanÄ±cÄ± mesajÄ±nÄ± ekle
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # Asistan yanÄ±tÄ±
            with st.chat_message("assistant"):
                with st.spinner(t("gemini_thinking")):
                    try:
                        # Gemini'den yanÄ±t al
                        response = get_gemini_response(
                            st.session_state.gemini_model,
                            prompt,
                            st.session_state.pdf_chunks,
                            st.session_state.messages[:-1]  # Son mesaj hariÃ§
                        )
                        
                        st.markdown(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    except Exception as e:
                        error_msg = f"{t('error_prefix')} {str(e)}"
                        st.error(error_msg)

                        # Hata tÃ¼rÃ¼ne gÃ¶re Ã¶neriler (kÄ±saltÄ±lmÄ±ÅŸ, lokalize)
                        error_str = str(e).lower()
                        if "429" in error_str or "quota" in error_str or "limit" in error_str:
                            st.warning(t('quota_suggestions'))
                        elif "api key" in error_str or "authentication" in error_str or "401" in error_str:
                            st.warning(t('invalid_key_suggestion'))
                        elif "safety" in error_str or "blocked" in error_str:
                            st.warning(t('safety_blocked'))
                        elif "404" in error_str or "not found" in error_str:
                            st.warning(t('model_not_found'))

                        st.session_state.messages.append({"role": "assistant", "content": error_msg})


# Footer
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    if st.session_state.pdf_chunks:
        st.metric("Metin ParÃ§alarÄ±", len(st.session_state.pdf_chunks))
with col2:
    if st.session_state.messages:
        st.metric("Sohbet MesajlarÄ±", len(st.session_state.messages))
with col3:
    st.metric("Aktif Model", selected_model.split('-')[1] if '-' in selected_model else selected_model)

st.markdown(t('footer_html'), unsafe_allow_html=True)