"""
PDF Belge AsistanÄ± - Google Gemini Edition (Optimize EdilmiÅŸ)
KullanÄ±cÄ±larÄ±n PDF dosyasÄ± yÃ¼kleyip sorular sorabileceÄŸi bir Streamlit uygulamasÄ±.
"""

import streamlit as st
from PyPDF2 import PdfReader
import google.generativeai as genai
import os
from dotenv import load_dotenv
import json
from datetime import datetime
import time

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# Sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="PDF Belge AsistanÄ±",
    page_icon="ğŸ“„",
    layout="wide"
)

# BaÅŸlÄ±k ve aÃ§Ä±klama
st.title("ğŸ“„ PDF Belge AsistanÄ±")
st.markdown("PDF dosyanÄ±zÄ± yÃ¼kleyin ve iÃ§eriÄŸi hakkÄ±nda sorular sorun! *(Google Gemini ile Ã§alÄ±ÅŸÄ±r)*")


def extract_text_from_pdf(pdf_file):
    """
    PDF dosyasÄ±ndan metin Ã§Ä±karÄ±r.
    
    Args:
        pdf_file: YÃ¼klenen PDF dosyasÄ±
        
    Returns:
        tuple: (metin, sayfa_sayÄ±sÄ±)
    """
    try:
        pdf_reader = PdfReader(pdf_file)
        text = ""
        page_count = len(pdf_reader.pages)
        
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            if page_text.strip():  # BoÅŸ sayfalarÄ± atla
                text += f"\n--- Sayfa {page_num} ---\n{page_text}"
        
        return text, page_count
    except Exception as e:
        st.error(f"PDF okunurken hata oluÅŸtu: {str(e)}")
        return None, 0


def get_text_stats(text):
    """
    Metin istatistiklerini hesaplar.
    
    Args:
        text: Analiz edilecek metin
        
    Returns:
        dict: Karakter ve kelime sayÄ±sÄ±
    """
    word_count = len(text.split())
    char_count = len(text)
    return {"words": word_count, "characters": char_count}


def chunk_text(text, max_chars=3000):
    """
    Metni kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler (token tasarrufu iÃ§in).
    
    Args:
        text: BÃ¶lÃ¼necek metin
        max_chars: Maksimum karakter sayÄ±sÄ±
        
    Returns:
        list: Metin parÃ§alarÄ±
    """
    chunks = []
    current_chunk = ""
    
    for line in text.split('\n'):
        if len(current_chunk) + len(line) < max_chars:
            current_chunk += line + '\n'
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = line + '\n'
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


def search_relevant_chunks(chunks, query, top_k=2):
    """
    Soruyla ilgili en alakalÄ± metin parÃ§alarÄ±nÄ± bulur (basit keyword arama).
    
    Args:
        chunks: Metin parÃ§alarÄ± listesi
        query: KullanÄ±cÄ± sorusu
        top_k: KaÃ§ parÃ§a dÃ¶ndÃ¼rÃ¼lecek
        
    Returns:
        str: BirleÅŸtirilmiÅŸ alakalÄ± metin parÃ§alarÄ±
    """
    query_words = set(query.lower().split())
    
    # Her chunk iÃ§in skor hesapla
    scored_chunks = []
    for chunk in chunks:
        chunk_words = set(chunk.lower().split())
        score = len(query_words & chunk_words)  # Ortak kelime sayÄ±sÄ±
        scored_chunks.append((score, chunk))
    
    # En yÃ¼ksek skorlu parÃ§alarÄ± al
    scored_chunks.sort(reverse=True, key=lambda x: x[0])
    relevant_chunks = [chunk for score, chunk in scored_chunks[:top_k] if score > 0]
    
    # EÄŸer hiÃ§ eÅŸleÅŸme yoksa ilk chunk'Ä± dÃ¶ndÃ¼r
    if not relevant_chunks and chunks:
        relevant_chunks = [chunks[0]]
    
    return '\n\n'.join(relevant_chunks)


def initialize_gemini(model_name, api_key):
    """
    Google Gemini modelini baÅŸlatÄ±r.
    
    Args:
        model_name: KullanÄ±lacak Gemini model adÄ±
        api_key: Google API anahtarÄ±
        
    Returns:
        GenerativeModel: YapÄ±landÄ±rÄ±lmÄ±ÅŸ Gemini modeli
    """
    try:
        genai.configure(api_key=api_key)
        # Model adÄ±na "models/" prefix'i ekle
        full_model_name = f"models/{model_name}" if not model_name.startswith("models/") else model_name
        
        # Optimized generation config
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 2048,  # Ã‡Ä±kÄ±ÅŸ token limiti
        }
        
        model = genai.GenerativeModel(
            full_model_name,
            generation_config=generation_config
        )
        return model
    except Exception as e:
        st.error(f"Model baÅŸlatÄ±lÄ±rken hata: {str(e)}")
        return None


def get_gemini_response(model, prompt, pdf_chunks, chat_history):
    """
    Gemini'den yanÄ±t alÄ±r (Optimize EdilmiÅŸ - Daha Az Token).
    
    Args:
        model: Gemini model instance
        prompt: KullanÄ±cÄ± sorusu
        pdf_chunks: PDF iÃ§eriÄŸi parÃ§alarÄ±
        chat_history: Sohbet geÃ§miÅŸi
        
    Returns:
        str: Model yanÄ±tÄ±
    """
    try:
        # Soruyla ilgili en alakalÄ± metinleri bul
        relevant_context = search_relevant_chunks(pdf_chunks, prompt, top_k=2)
        
        # Sadece son 2 sohbet turunu dahil et (token tasarrufu)
        recent_history = chat_history[-4:] if len(chat_history) > 4 else chat_history
        
        # KÄ±sa chat history formatla
        history_text = ""
        if recent_history:
            for msg in recent_history:
                role = "K" if msg["role"] == "user" else "A"
                # Uzun mesajlarÄ± kÄ±salt
                content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
                history_text += f"{role}: {content}\n"
        
        # KÄ±saltÄ±lmÄ±ÅŸ ve optimize edilmiÅŸ prompt
        system_prompt = """PDF belge asistanÄ±sÄ±n. Sadece verilen bilgilere gÃ¶re yanÄ±t ver.

Ä°lgili Metin:
{context}

{history}
Soru: {question}

YanÄ±t:"""

        # Prompt'u hazÄ±rla
        full_prompt = system_prompt.format(
            context=relevant_context[:3500],  # Daha az token
            history=f"Ã–nceki:\n{history_text}\n" if history_text else "",
            question=prompt
        )
        
        # GÃ¼venlik ayarlarÄ±
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
        ]
        
        # Rate limiting - her istekten Ã¶nce kÄ±sa bir bekleme
        if 'last_request_time' in st.session_state:
            elapsed = time.time() - st.session_state.last_request_time
            if elapsed < 2:  # 2 saniyeden kÄ±sa sÃ¼rede istek atÄ±lmÄ±ÅŸsa bekle
                time.sleep(2 - elapsed)
        
        st.session_state.last_request_time = time.time()
        
        # Gemini'den yanÄ±t al
        response = model.generate_content(
            full_prompt,
            safety_settings=safety_settings
        )
        
        return response.text
    
    except Exception as e:
        raise Exception(f"Gemini yanÄ±t hatasÄ±: {str(e)}")


def export_chat_history(messages, format_type="txt"):
    """
    Sohbet geÃ§miÅŸini dÄ±ÅŸa aktarÄ±r.
    
    Args:
        messages: Sohbet mesajlarÄ± listesi
        format_type: Dosya formatÄ± ("txt" veya "json")
        
    Returns:
        str: DÄ±ÅŸa aktarÄ±lacak iÃ§erik
    """
    if format_type == "txt":
        content = "PDF Belge AsistanÄ± - Sohbet GeÃ§miÅŸi\n"
        content += "=" * 50 + "\n"
        content += f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        for msg in messages:
            role = "KullanÄ±cÄ±" if msg["role"] == "user" else "Asistan"
            content += f"{role}: {msg['content']}\n\n"
        
        return content
    
    elif format_type == "json":
        export_data = {
            "export_date": datetime.now().isoformat(),
            "messages": messages
        }
        return json.dumps(export_data, ensure_ascii=False, indent=2)


# Session state baÅŸlatma
if "messages" not in st.session_state:
    st.session_state.messages = []

if "pdf_text" not in st.session_state:
    st.session_state.pdf_text = None

if "pdf_chunks" not in st.session_state:
    st.session_state.pdf_chunks = []

if "pdf_info" not in st.session_state:
    st.session_state.pdf_info = {}

if "gemini_model" not in st.session_state:
    st.session_state.gemini_model = None

if "last_request_time" not in st.session_state:
    st.session_state.last_request_time = 0


# Sidebar - Ayarlar ve Kontroller
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    
    # API Key kontrolÃ¼ - GEMINI
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        api_key = st.text_input(
            "Google Gemini API Key", 
            type="password", 
            help="API key'inizi .env dosyasÄ±na veya buraya girebilirsiniz"
        )
    
    if api_key:
        st.success("âœ… Gemini API Key yÃ¼klendi")
    else:
        st.warning("âš ï¸ LÃ¼tfen Gemini API Key girin")
    
    # Model seÃ§imi - GÃœNCEL GEMINI MODELLER
    st.subheader("ğŸ¤– Model SeÃ§imi")
    
    # GÃ¼ncel Gemini model kategorileri ve aÃ§Ä±klamalarÄ±
    model_info = {
        "gemini-flash-latest": "ğŸ’¨ Ultra hafif - En az token (Ã–NERÄ°LEN)",
        "gemini-1.5-flash": "âš¡ HÄ±zlÄ± ve dengeli",
        "gemini-2.0-flash-exp": "ğŸš€ Yeni deneysel model",
        "gemini-1.5-pro": "ğŸ’ En gÃ¼Ã§lÃ¼ (daha fazla token)"
    }
    
    selected_model = st.selectbox(
        "Model",
        list(model_info.keys()),
        index=0,
        format_func=lambda x: f"{x} - {model_info[x]}",
        help="Quota sorunu iÃ§in gemini-1.5-flash-8b Ã¶nerilir"
    )
    
    # Model bilgisi
    st.info(f"â„¹ï¸ SeÃ§ili: **{selected_model}**")
    
    # Optimizasyon bilgisi
    with st.expander("âš¡ Optimizasyon NotlarÄ±"):
        st.markdown("""
        **Token Tasarrufu Ä°Ã§in YapÄ±lanlar:**
        - âœ… AkÄ±llÄ± metin parÃ§alama (chunking)
        - âœ… Soruyla ilgili kÄ±sÄ±mlar aranÄ±yor
        - âœ… Sadece son 2 sohbet turunu gÃ¶nderme
        - âœ… 2 saniye rate limiting
        - âœ… KÄ±saltÄ±lmÄ±ÅŸ prompt formatÄ±
        - âœ… Maksimum 3500 karakter context
        
        **Ã–neriler:**
        - KÄ±sa ve net sorular sorun
        - gemini-1.5-flash-8b modelini kullanÄ±n
        - Ã‡ok uzun PDF'ler iÃ§in sorularÄ± spesifik yapÄ±n
        """)
    
    # API Key alma bilgisi
    with st.expander("ğŸ”‘ Gemini API Key nasÄ±l alÄ±nÄ±r?"):
        st.markdown("""
        **Gemini API Key Alma AdÄ±mlarÄ±:**
        1. [Google AI Studio](https://aistudio.google.com/app/apikey) sayfasÄ±na gidin
        2. Google hesabÄ±nÄ±zla giriÅŸ yapÄ±n
        3. "Get API Key" veya "Create API Key" butonuna tÄ±klayÄ±n
        4. API Key'i kopyalayÄ±n
        5. `.env` dosyasÄ±na `GEMINI_API_KEY=your_key_here` ÅŸeklinde ekleyin
        
        **Ãœcretsiz Limitler:**
        - 15 istek/dakika
        - 1500 istek/gÃ¼n
        - 1 milyon token/dakika (giriÅŸ)
        """)
    
    st.divider()
    
    # PDF yÃ¼kleme
    st.subheader("ğŸ“¤ PDF YÃ¼kle")
    uploaded_file = st.file_uploader(
        "PDF DosyasÄ± SeÃ§in",
        type=["pdf"],
        help="Maksimum 10MB boyutunda PDF yÃ¼kleyebilirsiniz"
    )
    
    # Dosya boyutu kontrolÃ¼
    if uploaded_file is not None:
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        if file_size_mb > 10:
            st.error("âŒ Dosya boyutu 10MB'dan bÃ¼yÃ¼k olamaz!")
            uploaded_file = None
        else:
            st.info(f"ğŸ“Š Dosya boyutu: {file_size_mb:.2f} MB")
            
            # PDF iÅŸleme
            if st.button("ğŸ“– PDF'i Ä°ÅŸle", type="primary"):
                with st.spinner("PDF okunuyor..."):
                    text, page_count = extract_text_from_pdf(uploaded_file)
                    
                    if text:
                        st.session_state.pdf_text = text
                        
                        # Metni parÃ§alara bÃ¶l
                        with st.spinner("Metin parÃ§alanÄ±yor..."):
                            chunks = chunk_text(text, max_chars=3000)
                            st.session_state.pdf_chunks = chunks
                        
                        st.session_state.pdf_info = {
                            "filename": uploaded_file.name,
                            "pages": page_count,
                            "stats": get_text_stats(text),
                            "chunks": len(chunks)
                        }
                        
                        # Gemini modelini baÅŸlat
                        if api_key:
                            with st.spinner(f"{selected_model} baÅŸlatÄ±lÄ±yor..."):
                                model = initialize_gemini(selected_model, api_key)
                                if model:
                                    st.session_state.gemini_model = model
                                    st.success(f"âœ… PDF yÃ¼klendi! ({page_count} sayfa, {len(chunks)} parÃ§a)")
                                else:
                                    st.error("âŒ Model baÅŸlatÄ±lamadÄ±. API Key'inizi kontrol edin.")
                        else:
                            st.error("âŒ LÃ¼tfen Gemini API Key girin!")
                        
                        if st.session_state.gemini_model:
                            st.rerun()
    
    # PDF bilgileri
    if st.session_state.pdf_text:
        st.divider()
        st.subheader("ğŸ“‹ Belge Bilgileri")
        st.write(f"**Dosya:** {st.session_state.pdf_info['filename']}")
        st.write(f"**Sayfa SayÄ±sÄ±:** {st.session_state.pdf_info['pages']}")
        st.write(f"**Kelime SayÄ±sÄ±:** {st.session_state.pdf_info['stats']['words']:,}")
        st.write(f"**Metin ParÃ§alarÄ±:** {st.session_state.pdf_info['chunks']}")
        
        # Token tahmini
        estimated_tokens = st.session_state.pdf_info['stats']['characters'] // 4
        st.write(f"**Tahmini Token:** ~{estimated_tokens:,}")
        
        # PDF Ã¶nizleme
        with st.expander("ğŸ‘ï¸ Metin Ã–nizleme"):
            preview_text = st.session_state.pdf_text[:500] + "..."
            st.text_area("Ä°lk 500 karakter", preview_text, height=150, disabled=True)
    
    # Sohbet kontrolÃ¼
    if st.session_state.messages:
        st.divider()
        st.subheader("ğŸ’¬ Sohbet KontrolÃ¼")
        
        st.info(f"ğŸ“Š {len(st.session_state.messages)} mesaj")
        
        # Sohbeti temizle
        if st.button("ğŸ—‘ï¸ Sohbeti Temizle", type="secondary"):
            st.session_state.messages = []
            st.rerun()
        
        # Sohbeti indir
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                label="ğŸ“„ TXT",
                data=export_chat_history(st.session_state.messages, "txt"),
                file_name=f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                use_container_width=True
            )
        with col2:
            st.download_button(
                label="ğŸ“‹ JSON",
                data=export_chat_history(st.session_state.messages, "json"),
                file_name=f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )


# Ana alan - Sohbet
if not st.session_state.pdf_text:
    st.info("ğŸ‘ˆ BaÅŸlamak iÃ§in sol taraftan bir PDF dosyasÄ± yÃ¼kleyin")
elif not st.session_state.gemini_model:
    st.warning("âš ï¸ Model baÅŸlatÄ±lamadÄ±. LÃ¼tfen Gemini API Key'inizi kontrol edip PDF'i tekrar iÅŸleyin.")
else:
    # Sohbet geÃ§miÅŸini gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # KullanÄ±cÄ± giriÅŸi
    if prompt := st.chat_input("PDF hakkÄ±nda bir soru sorun..."):
        if not api_key:
            st.error("âŒ LÃ¼tfen Ã¶nce Gemini API Key girin!")
        else:
            # KullanÄ±cÄ± mesajÄ±nÄ± ekle
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # Asistan yanÄ±tÄ±
            with st.chat_message("assistant"):
                with st.spinner("Gemini dÃ¼ÅŸÃ¼nÃ¼yor..."):
                    try:
                        # Gemini'den yanÄ±t al
                        response = get_gemini_response(
                            st.session_state.gemini_model,
                            prompt,
                            st.session_state.pdf_chunks,
                            st.session_state.messages[:-1]  # Son mesaj hariÃ§
                        )
                        
                        st.markdown(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    except Exception as e:
                        error_msg = f"âŒ Hata oluÅŸtu: {str(e)}"
                        st.error(error_msg)
                        
                        # Hata tÃ¼rÃ¼ne gÃ¶re Ã¶neriler
                        error_str = str(e).lower()
                        if "429" in error_str or "quota" in error_str or "limit" in error_str:
                            st.warning("""
                            ğŸ’¡ **Quota AÅŸÄ±ldÄ± - Ã‡Ã¶zÃ¼m Ã–nerileri:**
                            
                            1. **gemini-1.5-flash-8b** modelini kullanÄ±n (en az token tÃ¼ketir)
                            2. BirkaÃ§ saniye bekleyip tekrar deneyin
                            3. Daha **kÄ±sa ve spesifik** sorular sorun
                            4. PDF'nizin boyutunu kÃ¼Ã§Ã¼ltÃ¼n
                            5. Sohbet geÃ§miÅŸini temizleyin
                            6. FarklÄ± bir API key deneyin
                            7. GÃ¼nlÃ¼k limitiniz dolmuÅŸsa yarÄ±n tekrar deneyin
                            
                            **Not:** Bu uygulama token tasarrufu iÃ§in optimize edildi.
                            """)
                        elif "api key" in error_str or "authentication" in error_str or "401" in error_str:
                            st.warning("ğŸ’¡ API Key'iniz geÃ§ersiz olabilir. [Google AI Studio](https://aistudio.google.com/app/apikey) Ã¼zerinden yeni bir key alÄ±n.")
                        elif "safety" in error_str or "blocked" in error_str:
                            st.warning("ğŸ’¡ Gemini gÃ¼venlik filtresi iÃ§eriÄŸi engelledi. Sorunuzu farklÄ± ÅŸekilde ifade edin.")
                        elif "404" in error_str or "not found" in error_str:
                            st.warning("ğŸ’¡ Model bulunamadÄ±. **gemini-1.5-flash-8b** modelini deneyin.")
                        
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})


# Footer
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    if st.session_state.pdf_chunks:
        st.metric("Metin ParÃ§alarÄ±", len(st.session_state.pdf_chunks))
with col2:
    if st.session_state.messages:
        st.metric("Sohbet MesajlarÄ±", len(st.session_state.messages))
with col3:
    st.metric("Aktif Model", selected_model.split('-')[1] if '-' in selected_model else selected_model)

st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 0.8em; margin-top: 10px;'>
    ğŸ“„ PDF Belge AsistanÄ± v2.1 (Optimize EdilmiÅŸ) | Powered by Google Gemini<br>
    <small>Token tasarrufu iÃ§in optimize edildi â€¢ <a href="https://aistudio.google.com/app/apikey" target="_blank">API Key Al</a></small>
    </div>
    """,
    unsafe_allow_html=True
)